<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dynamical Systems | Efstratios Gavves</title>
    <link>https://egavves.github.io/tag/dynamical-systems/</link>
      <atom:link href="https://egavves.github.io/tag/dynamical-systems/index.xml" rel="self" type="application/rss+xml" />
    <description>Dynamical Systems</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://egavves.github.io/media/icon_huac07b9ebb8ffd2b1d1617642febbe690_115450_512x512_fill_lanczos_center_3.png</url>
      <title>Dynamical Systems</title>
      <link>https://egavves.github.io/tag/dynamical-systems/</link>
    </image>
    
    <item>
      <title>ERC Starting Grant</title>
      <link>https://egavves.github.io/project/eva/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/eva/</guid>
      <description>&lt;p&gt;Visual artificial intelligence automatically interprets what happens in visual data like videos. Today’s research strives with queries like: &lt;em&gt;&amp;ldquo;Is this person playing basketball?&amp;quot;&lt;/em&gt;; &lt;em&gt;&amp;ldquo;Find the location of the brain stroke&amp;rdquo;&lt;/em&gt;; or &lt;em&gt;&amp;ldquo;Track the glacier fractures in satellite footage&amp;rdquo;&lt;/em&gt;. All these queries are about visual observations already taken place. Today’s algorithms focus on explaining past visual observations. Naturally, not all queries are about the past: &lt;em&gt;&amp;ldquo;Will this person draw something in or out of their pocket?&amp;quot;&lt;/em&gt;; &lt;em&gt;&amp;ldquo;Where will the tumour be in 5 seconds given breathing patterns and moving organs?&amp;quot;&lt;/em&gt;; or, &lt;em&gt;&amp;ldquo;How will the glacier fracture given the current motion and melting patterns?&amp;quot;&lt;/em&gt;. For these queries and all others, the next generation of visual algorithms must expect what happens next given past visual observations. Visual artificial intelligence must also be able to prevent before the fact, rather than explain only after it. I propose an ambitious 5-year project to design algorithms that learn to expect the possible futures from visual sequences.&lt;/p&gt;
&lt;p&gt;The main challenge for expecting possible futures is having visual algorithms that learn temporality in visual sequences. Today’s algorithms cannot do this convincingly. First, they are time-deterministic and ignore uncertainty, part of any expected future. I propose time-stochastic visual algorithms. Second, today’s algorithms are time-extrinsic and treat time as an external input or output variable. I propose time-intrinsic visual algorithms that integrate time within their latent representations. Third, visual algorithms must account for all innumerable spatiotemporal dynamics, despite their finite nature. I propose time-geometric visual algorithms that constrain temporal latent spaces to known geometries.&lt;/p&gt;
&lt;p&gt;EVA addresses fundamental research issues in the automatic interpretation of future visual sequences. Its results will serve as a basis for ground-breaking technological advances in practical vision applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NWO VIDI</title>
      <link>https://egavves.github.io/project/timing/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/timing/</guid>
      <description>&lt;p&gt;From Facebook’s 3.5 billion live streams to the complex MRI sequences and satellite footage monitoring glaciers, video recognition becomes increasingly relevant. Ultimately it will enable to understand what is happening, where and when in videos by artificial intelligence. Encouraged by the breakthrough of deep representation learning in static image recognition, today&amp;rsquo;s video recognition algorithms emphasize static representations. In effect, they are time invariant. Ignoring time like this suffices in simple short videos, but in tomorrow’s applications recognizing time is imperative: it determines whether a suspect draws something in or out of their pocket, where a tumour will move in the MRI or at which rate glaciers melt in satellite footage. For all these cases and more, video algorithms must be time equivariant, that is yield representations that change proportionally to the temporal change in the input. As we move to video understanding where temporality is critical, time equivariant algorithms are a must.&lt;/p&gt;
&lt;p&gt;This is a 5-year research program that studies, develops and evaluates time equivariant video algorithms. To achieve this, we will approach video algorithms from two angles: time geometry, and time supervision. Geometry helps with accounting for innumerable patterns without blowing up the representation complexity. Time supervision helps with learning time equivariance, without relying on strong manual supervision. A temporal decathlon competition will be introduced to the community to evaluate, disseminate and utilize the temporal behaviour of video algorithms. The decathlon will serve as a proxy for designing better video algorithms more efficiently. It will also open up video algorithms to other disciplines, where researchers have videos and know their temporal properties but do not have a common reference point. All research will be published in the top relevant conferences and journals. The major innovation of the proposed research is understanding and exploiting time in video algorithms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IvI grant</title>
      <link>https://egavves.github.io/project/nnds/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/nnds/</guid>
      <description>&lt;p&gt;Artificial Intelligence has repeatedly been breaking records in core machine learning and computer vision tasks including object recognition, increasing neural network depth, and action classification, mainly due to the success of deep neural networks. Despite their success, the inherent complexity of deep neural networks renders them opaque to in-depth understanding of how their complex capabilities arise from the simple dynamics of the artificial neurons. As a consequence, deep networks are often associated with lack of explainability of predictions, instability, or even lack of transparency when it comes to improving neural network building blocks.&lt;/p&gt;
&lt;p&gt;In this project, deep neural networks (DNN) will be studied in the context of complex adaptive systems analysis. The goal is to gain insights into the structural and functional properties of the neural network computational graph resulting from the learning process. Techniques that will be employed include dynamical systems theory and iterative maps (chaotic attractors; Lyapunov exponent), information theory (Shannon entropy, mutual information, multivariate measures such as synergistic information), and network theory. Overarching questions include: How do these (multilayer) networks self-organize to solve a particular task? How is information represented in these systems? Is there a set of fundamental properties underlying the structure and dynamics of deep neural networks?&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
