<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Efstratios Gavves</title>
    <link>https://egavves.github.io/project/</link>
      <atom:link href="https://egavves.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://egavves.github.io/media/icon_huac07b9ebb8ffd2b1d1617642febbe690_115450_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://egavves.github.io/project/</link>
    </image>
    
    <item>
      <title>ERC Starting Grant</title>
      <link>https://egavves.github.io/project/eva/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/eva/</guid>
      <description>&lt;p&gt;Visual artificial intelligence automatically interprets what happens in visual data like videos. Today’s research strives with queries like: &lt;em&gt;&amp;ldquo;Is this person playing basketball?&amp;quot;&lt;/em&gt;; &lt;em&gt;&amp;ldquo;Find the location of the brain stroke&amp;rdquo;&lt;/em&gt;; or &lt;em&gt;&amp;ldquo;Track the glacier fractures in satellite footage&amp;rdquo;&lt;/em&gt;. All these queries are about visual observations already taken place. Today’s algorithms focus on explaining past visual observations. Naturally, not all queries are about the past: &lt;em&gt;&amp;ldquo;Will this person draw something in or out of their pocket?&amp;quot;&lt;/em&gt;; &lt;em&gt;&amp;ldquo;Where will the tumour be in 5 seconds given breathing patterns and moving organs?&amp;quot;&lt;/em&gt;; or, &lt;em&gt;&amp;ldquo;How will the glacier fracture given the current motion and melting patterns?&amp;quot;&lt;/em&gt;. For these queries and all others, the next generation of visual algorithms must expect what happens next given past visual observations. Visual artificial intelligence must also be able to prevent before the fact, rather than explain only after it. I propose an ambitious 5-year project to design algorithms that learn to expect the possible futures from visual sequences.&lt;/p&gt;
&lt;p&gt;The main challenge for expecting possible futures is having visual algorithms that learn temporality in visual sequences. Today’s algorithms cannot do this convincingly. First, they are time-deterministic and ignore uncertainty, part of any expected future. I propose time-stochastic visual algorithms. Second, today’s algorithms are time-extrinsic and treat time as an external input or output variable. I propose time-intrinsic visual algorithms that integrate time within their latent representations. Third, visual algorithms must account for all innumerable spatiotemporal dynamics, despite their finite nature. I propose time-geometric visual algorithms that constrain temporal latent spaces to known geometries.&lt;/p&gt;
&lt;p&gt;EVA addresses fundamental research issues in the automatic interpretation of future visual sequences. Its results will serve as a basis for ground-breaking technological advances in practical vision applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NWO VIDI</title>
      <link>https://egavves.github.io/project/timing/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/timing/</guid>
      <description>&lt;p&gt;From Facebook’s 3.5 billion live streams to the complex MRI sequences and satellite footage monitoring glaciers, video recognition becomes increasingly relevant. Ultimately it will enable to understand what is happening, where and when in videos by artificial intelligence. Encouraged by the breakthrough of deep representation learning in static image recognition, today&amp;rsquo;s video recognition algorithms emphasize static representations. In effect, they are time invariant. Ignoring time like this suffices in simple short videos, but in tomorrow’s applications recognizing time is imperative: it determines whether a suspect draws something in or out of their pocket, where a tumour will move in the MRI or at which rate glaciers melt in satellite footage. For all these cases and more, video algorithms must be time equivariant, that is yield representations that change proportionally to the temporal change in the input. As we move to video understanding where temporality is critical, time equivariant algorithms are a must.&lt;/p&gt;
&lt;p&gt;This is a 5-year research program that studies, develops and evaluates time equivariant video algorithms. To achieve this, we will approach video algorithms from two angles: time geometry, and time supervision. Geometry helps with accounting for innumerable patterns without blowing up the representation complexity. Time supervision helps with learning time equivariance, without relying on strong manual supervision. A temporal decathlon competition will be introduced to the community to evaluate, disseminate and utilize the temporal behaviour of video algorithms. The decathlon will serve as a proxy for designing better video algorithms more efficiently. It will also open up video algorithms to other disciplines, where researchers have videos and know their temporal properties but do not have a common reference point. All research will be published in the top relevant conferences and journals. The major innovation of the proposed research is understanding and exploiting time in video algorithms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>QUVA</title>
      <link>https://egavves.github.io/project/quva/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/quva/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>POP-AART</title>
      <link>https://egavves.github.io/project/popaart/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/popaart/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IvI grant</title>
      <link>https://egavves.github.io/project/nnds/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/nnds/</guid>
      <description>&lt;p&gt;Artificial Intelligence has repeatedly been breaking records in core machine learning and computer vision tasks including object recognition, increasing neural network depth, and action classification, mainly due to the success of deep neural networks. Despite their success, the inherent complexity of deep neural networks renders them opaque to in-depth understanding of how their complex capabilities arise from the simple dynamics of the artificial neurons. As a consequence, deep networks are often associated with lack of explainability of predictions, instability, or even lack of transparency when it comes to improving neural network building blocks.&lt;/p&gt;
&lt;p&gt;In this project, deep neural networks (DNN) will be studied in the context of complex adaptive systems analysis. The goal is to gain insights into the structural and functional properties of the neural network computational graph resulting from the learning process. Techniques that will be employed include dynamical systems theory and iterative maps (chaotic attractors; Lyapunov exponent), information theory (Shannon entropy, mutual information, multivariate measures such as synergistic information), and network theory. Overarching questions include: How do these (multilayer) networks self-organize to solve a particular task? How is information represented in these systems? Is there a set of fundamental properties underlying the structure and dynamics of deep neural networks?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NWO LIFT</title>
      <link>https://egavves.github.io/project/flora/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/flora/</guid>
      <description>&lt;p&gt;In today’s deep learning era in computer vision and machine learning, computers rival humans in what were once considered to be human-only tasks, with impressive results in object scene recognition, semantic segmentation and action classification. In this setting automotive companies, including BMW, Toyota and Tesla to name just three examples, have taken up the challenge to use deep learning technologies to design the next generation of vehicles, namely fully autonomous, self-driving cars. Current vehicles already enjoy conditional automation (level 3 out of 5), where the vehicle can drive autonomously for as long as no unexpected situations happen. However, the ultimate goal is to arrive at vehicles that drive fully autonomously (level 5 out of 5), where cars navigate without human intervention, avoiding collisions while driving comfortably and being socially aware of their surroundings. Going from conditional to full automation, however, is disproportionally hard, as the recent fatal accidents by the experimental Tesla and Uber autonomous vehicles have shown. Significant academic and industrial research efforts are necessary to reach autonomous vehicles with full automation levels that can predict and avoid collisions, while driving comfortably.&lt;/p&gt;
&lt;p&gt;For research in collision avoidance, predicting future trajectories of traffic participants is of the utmost important. Unfortunately, current collision checkers operate under the assumption of a static world where objects do not move. Especially important are the most vulnerable traffic participants, like pedestrians, bicyclists and motorcyclists, who unlike cars, are highly manoeuvrable and cannot be modelled by standard techniques. Predicting the future trajectories of vulnerable traffic participants is crucial, as they run the highest risk of getting injured in accidents. In Germany alone 2.5 million traffic accidents occurred in 2016, including 65,000 heavy injuries and 3,000 fatalities, while in the Netherlands there were 613 fatalities in traffic accidents in 2017, 407 involving cyclists and motorcyclists. This proposal places special importance in predicting future trajectories of vulnerable traffic participants.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TKI-NKI</title>
      <link>https://egavves.github.io/project/histo-ai/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/histo-ai/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TKI-AMC</title>
      <link>https://egavves.github.io/project/airborne/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/airborne/</guid>
      <description>&lt;p&gt;Stroke is the leading cause of disability in adults and every year 40,000 patients suffer stroke in the Netherlands. Large vessel occlusions (LVO) disproportionally contribute to stroke-related dependence and death. After publication of the Dutch MR CLEAN trial in 2015, endovascular thrombectomy (EVT) has been described as one of the biggest breakthroughs in modern medicine. EVT for ischemic stroke patients with LVO arriving within 6 hours increases the odds of achieving a good outcome nearly 2.5 fold. Imaging based treatment selection promises equal effectiveness of EVT for LVO stroke in patients arriving later or for wake-up strokes and requires information on the volume of salvageable and infarcted tissue.&lt;/p&gt;
&lt;p&gt;Assessment of salvageable and infarcted tissue with CT perfusion (CTP) is possible but difficult. Most primary stroke centres (PSCs) cannot perform and interpret CTP necessitating transfer from PSCs to comprehensive stroke centres (CSCs). Conversely, non-contrast CT (NCCT) and CT angiography (CTA) are currently available in all PSCs.&lt;/p&gt;
&lt;p&gt;The aim of AIRBORNE is to develop AI-based tooling for estimation of infarct volume on NCCT and CTA scans using deep learning neural networks making CTP unnecessary. Early treatment selection will speed up transfers and decrease onset to treatment times, improving the outcome of treated patients
This project contributes to topsector Life Sciences &amp;amp; Health goals by applying AI algorithms in cloud-based decision tools to improve outcomes and healthcare efficacy while gaining insight into data efficient AI.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
