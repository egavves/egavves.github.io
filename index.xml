<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Efstratios Gavves</title>
    <link>https://egavves.github.io/</link>
      <atom:link href="https://egavves.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Efstratios Gavves</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://egavves.github.io/media/icon_huac07b9ebb8ffd2b1d1617642febbe690_115450_512x512_fill_lanczos_center_3.png</url>
      <title>Efstratios Gavves</title>
      <link>https://egavves.github.io/</link>
    </image>
    
    <item>
      <title>ERC Starting Grant</title>
      <link>https://egavves.github.io/project/eva/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/eva/</guid>
      <description>&lt;p&gt;Visual artificial intelligence automatically interprets what happens in visual data like videos. Today‚Äôs research strives with queries like: &lt;em&gt;&amp;ldquo;Is this person playing basketball?&amp;quot;&lt;/em&gt;; &lt;em&gt;&amp;ldquo;Find the location of the brain stroke&amp;rdquo;&lt;/em&gt;; or &lt;em&gt;&amp;ldquo;Track the glacier fractures in satellite footage&amp;rdquo;&lt;/em&gt;. All these queries are about visual observations already taken place. Today‚Äôs algorithms focus on explaining past visual observations. Naturally, not all queries are about the past: &lt;em&gt;&amp;ldquo;Will this person draw something in or out of their pocket?&amp;quot;&lt;/em&gt;; &lt;em&gt;&amp;ldquo;Where will the tumour be in 5 seconds given breathing patterns and moving organs?&amp;quot;&lt;/em&gt;; or, &lt;em&gt;&amp;ldquo;How will the glacier fracture given the current motion and melting patterns?&amp;quot;&lt;/em&gt;. For these queries and all others, the next generation of visual algorithms must expect what happens next given past visual observations. Visual artificial intelligence must also be able to prevent before the fact, rather than explain only after it. I propose an ambitious 5-year project to design algorithms that learn to expect the possible futures from visual sequences.&lt;/p&gt;
&lt;p&gt;The main challenge for expecting possible futures is having visual algorithms that learn temporality in visual sequences. Today‚Äôs algorithms cannot do this convincingly. First, they are time-deterministic and ignore uncertainty, part of any expected future. I propose time-stochastic visual algorithms. Second, today‚Äôs algorithms are time-extrinsic and treat time as an external input or output variable. I propose time-intrinsic visual algorithms that integrate time within their latent representations. Third, visual algorithms must account for all innumerable spatiotemporal dynamics, despite their finite nature. I propose time-geometric visual algorithms that constrain temporal latent spaces to known geometries.&lt;/p&gt;
&lt;p&gt;EVA addresses fundamental research issues in the automatic interpretation of future visual sequences. Its results will serve as a basis for ground-breaking technological advances in practical vision applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NWO VIDI</title>
      <link>https://egavves.github.io/project/timing/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/timing/</guid>
      <description>&lt;p&gt;From Facebook‚Äôs 3.5 billion live streams to the complex MRI sequences and satellite footage monitoring glaciers, video recognition becomes increasingly relevant. Ultimately it will enable to understand what is happening, where and when in videos by artificial intelligence. Encouraged by the breakthrough of deep representation learning in static image recognition, today&amp;rsquo;s video recognition algorithms emphasize static representations. In effect, they are time invariant. Ignoring time like this suffices in simple short videos, but in tomorrow‚Äôs applications recognizing time is imperative: it determines whether a suspect draws something in or out of their pocket, where a tumour will move in the MRI or at which rate glaciers melt in satellite footage. For all these cases and more, video algorithms must be time equivariant, that is yield representations that change proportionally to the temporal change in the input. As we move to video understanding where temporality is critical, time equivariant algorithms are a must.&lt;/p&gt;
&lt;p&gt;This is a 5-year research program that studies, develops and evaluates time equivariant video algorithms. To achieve this, we will approach video algorithms from two angles: time geometry, and time supervision. Geometry helps with accounting for innumerable patterns without blowing up the representation complexity. Time supervision helps with learning time equivariance, without relying on strong manual supervision. A temporal decathlon competition will be introduced to the community to evaluate, disseminate and utilize the temporal behaviour of video algorithms. The decathlon will serve as a proxy for designing better video algorithms more efficiently. It will also open up video algorithms to other disciplines, where researchers have videos and know their temporal properties but do not have a common reference point. All research will be published in the top relevant conferences and journals. The major innovation of the proposed research is understanding and exploiting time in video algorithms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>QUVA</title>
      <link>https://egavves.github.io/project/quva/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/quva/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>POP-AART</title>
      <link>https://egavves.github.io/project/popaart/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/popaart/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IvI grant</title>
      <link>https://egavves.github.io/project/nnds/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/nnds/</guid>
      <description>&lt;p&gt;Artificial Intelligence has repeatedly been breaking records in core machine learning and computer vision tasks including object recognition, increasing neural network depth, and action classification, mainly due to the success of deep neural networks. Despite their success, the inherent complexity of deep neural networks renders them opaque to in-depth understanding of how their complex capabilities arise from the simple dynamics of the artificial neurons. As a consequence, deep networks are often associated with lack of explainability of predictions, instability, or even lack of transparency when it comes to improving neural network building blocks.&lt;/p&gt;
&lt;p&gt;In this project, deep neural networks (DNN) will be studied in the context of complex adaptive systems analysis. The goal is to gain insights into the structural and functional properties of the neural network computational graph resulting from the learning process. Techniques that will be employed include dynamical systems theory and iterative maps (chaotic attractors; Lyapunov exponent), information theory (Shannon entropy, mutual information, multivariate measures such as synergistic information), and network theory. Overarching questions include: How do these (multilayer) networks self-organize to solve a particular task? How is information represented in these systems? Is there a set of fundamental properties underlying the structure and dynamics of deep neural networks?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NWO LIFT</title>
      <link>https://egavves.github.io/project/flora/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/flora/</guid>
      <description>&lt;p&gt;In today‚Äôs deep learning era in computer vision and machine learning, computers rival humans in what were once considered to be human-only tasks, with impressive results in object scene recognition, semantic segmentation and action classification. In this setting automotive companies, including BMW, Toyota and Tesla to name just three examples, have taken up the challenge to use deep learning technologies to design the next generation of vehicles, namely fully autonomous, self-driving cars. Current vehicles already enjoy conditional automation (level 3 out of 5), where the vehicle can drive autonomously for as long as no unexpected situations happen. However, the ultimate goal is to arrive at vehicles that drive fully autonomously (level 5 out of 5), where cars navigate without human intervention, avoiding collisions while driving comfortably and being socially aware of their surroundings. Going from conditional to full automation, however, is disproportionally hard, as the recent fatal accidents by the experimental Tesla and Uber autonomous vehicles have shown. Significant academic and industrial research efforts are necessary to reach autonomous vehicles with full automation levels that can predict and avoid collisions, while driving comfortably.&lt;/p&gt;
&lt;p&gt;For research in collision avoidance, predicting future trajectories of traffic participants is of the utmost important. Unfortunately, current collision checkers operate under the assumption of a static world where objects do not move. Especially important are the most vulnerable traffic participants, like pedestrians, bicyclists and motorcyclists, who unlike cars, are highly manoeuvrable and cannot be modelled by standard techniques. Predicting the future trajectories of vulnerable traffic participants is crucial, as they run the highest risk of getting injured in accidents. In Germany alone 2.5 million traffic accidents occurred in 2016, including 65,000 heavy injuries and 3,000 fatalities, while in the Netherlands there were 613 fatalities in traffic accidents in 2017, 407 involving cyclists and motorcyclists. This proposal places special importance in predicting future trajectories of vulnerable traffic participants.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TKI-NKI</title>
      <link>https://egavves.github.io/project/histo-ai/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/histo-ai/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TKI-AMC</title>
      <link>https://egavves.github.io/project/airborne/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/project/airborne/</guid>
      <description>&lt;p&gt;Stroke is the leading cause of disability in adults and every year 40,000 patients suffer stroke in the Netherlands. Large vessel occlusions (LVO) disproportionally contribute to stroke-related dependence and death. After publication of the Dutch MR CLEAN trial in 2015, endovascular thrombectomy (EVT) has been described as one of the biggest breakthroughs in modern medicine. EVT for ischemic stroke patients with LVO arriving within 6 hours increases the odds of achieving a good outcome nearly 2.5 fold. Imaging based treatment selection promises equal effectiveness of EVT for LVO stroke in patients arriving later or for wake-up strokes and requires information on the volume of salvageable and infarcted tissue.&lt;/p&gt;
&lt;p&gt;Assessment of salvageable and infarcted tissue with CT perfusion (CTP) is possible but difficult. Most primary stroke centres (PSCs) cannot perform and interpret CTP necessitating transfer from PSCs to comprehensive stroke centres (CSCs). Conversely, non-contrast CT (NCCT) and CT angiography (CTA) are currently available in all PSCs.&lt;/p&gt;
&lt;p&gt;The aim of AIRBORNE is to develop AI-based tooling for estimation of infarct volume on NCCT and CTA scans using deep learning neural networks making CTP unnecessary. Early treatment selection will speed up transfers and decrease onset to treatment times, improving the outcome of treated patients
This project contributes to topsector Life Sciences &amp;amp; Health goals by applying AI algorithms in cloud-based decision tools to improve outcomes and healthcare efficacy while gaining insight into data efficient AI.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://egavves.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BISCUIT: Causal Representation Learning from Binary Interactions</title>
      <link>https://egavves.github.io/publication/lippe-2023-biscuit/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/lippe-2023-biscuit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graph Switching Dynamical Systems</title>
      <link>https://egavves.github.io/publication/liu-2023-grass/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/liu-2023-grass/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Latent Field Discovery In Interacting Dynamical Systems With Neural Fields</title>
      <link>https://egavves.github.io/publication/kofinas-2023-aether/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/kofinas-2023-aether/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling Long Range Dependencies in ND: From Task-Specific to a General Purpose CNN</title>
      <link>https://egavves.github.io/publication/devries-2023-perfunet/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/devries-2023-perfunet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modulated Neural ODEs</title>
      <link>https://egavves.github.io/publication/auzina-2023-monode/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/auzina-2023-monode/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Modulation Fields for Conditional Cone Beam Neural Tomography</title>
      <link>https://egavves.github.io/publication/papa-2023-neuralct/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/papa-2023-neuralct/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT</title>
      <link>https://egavves.github.io/publication/papa-2023-noise-2-aliasing/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/papa-2023-noise-2-aliasing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PC-Reg: A pyramidal prediction‚Äìcorrection approach for large deformation image registration</title>
      <link>https://egavves.github.io/publication/yin-2023-pcreg/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/yin-2023-pcreg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spatio-temporal physics-informed learning: A novel approach to CT perfusion analysis in acute ischemic stroke</title>
      <link>https://egavves.github.io/publication/devries-2023-sppinn/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/devries-2023-sppinn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Time does tell: Self-supervised time-tuning of dense image representations</title>
      <link>https://egavves.github.io/publication/salehi-2023-time/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/salehi-2023-time/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards open-vocabulary video instance segmentation</title>
      <link>https://egavves.github.io/publication/wang-2023-towards/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/wang-2023-towards/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CITRIS: Causal Identifiability from Temporal Intervened Sequences</title>
      <link>https://egavves.github.io/publication/lippe-2022-citris/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/lippe-2022-citris/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Continual Learning of Dynamical Systems With Competitive Multi-Head Reservoirs</title>
      <link>https://egavves.github.io/publication/bereska-2022-continualdynamics/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/bereska-2022-continualdynamics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Learning-Based Thrombus Localization and Segmentation in Patients with Posterior Circulation Stroke</title>
      <link>https://egavves.github.io/publication/zoetmulder-2022-thrombus/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/zoetmulder-2022-thrombus/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&amp;E whole-slide images</title>
      <link>https://egavves.github.io/publication/schirris-2022-deepsmile/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/schirris-2022-deepsmile/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic Prototype Convolution Network for Few-Shot Semantic Segmentation</title>
      <link>https://egavves.github.io/publication/liu-2022-prototypes/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/liu-2022-prototypes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient Neural Causal Discovery without Acyclicity Constraints</title>
      <link>https://egavves.github.io/publication/lippe-2022-enco/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/lippe-2022-enco/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NFormer: Robust Person Re-Identification With Neighbor Transformer</title>
      <link>https://egavves.github.io/publication/wang-2022-nformer/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/wang-2022-nformer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Stability Regularization for Discrete Representation Learning</title>
      <link>https://egavves.github.io/publication/pervez-2022-stabreg/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/pervez-2022-stabreg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automated final lesion seentation in posterior circulation acute ischemic stroke using deep learning</title>
      <link>https://egavves.github.io/publication/zoetmulder-2021-automated/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/zoetmulder-2021-automated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Batch Bayesian Optimization on Permutations using Acquisition Weighted Kernels</title>
      <link>https://egavves.github.io/publication/oh-2021-batch/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/oh-2021-batch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&amp;E whole-slide images</title>
      <link>https://egavves.github.io/publication/schirris-2021-deepsmile/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/schirris-2021-deepsmile/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient Neural Causal Discovery without Acyclicity Constraints</title>
      <link>https://egavves.github.io/publication/lippe-2021-efficient/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/lippe-2021-efficient/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Federated mixture of experts</title>
      <link>https://egavves.github.io/publication/reisser-2021-federated/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/reisser-2021-federated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mixed Variable Bayesian Optimization with Frequency Modulated Kernels</title>
      <link>https://egavves.github.io/publication/oh-2021-mixed/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/oh-2021-mixed/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Model decay in long-term tracking</title>
      <link>https://egavves.github.io/publication/gavves-2021-model/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gavves-2021-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple-instance learning for assessing prognosis of ductal carcinoma in situ</title>
      <link>https://egavves.github.io/publication/dal-2021-multiple/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/dal-2021-multiple/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Feature Matching in Implicit 3D Representations</title>
      <link>https://egavves.github.io/publication/chen-2021-neural/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/chen-2021-neural/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quasibinary Classifier for Images with Zero and Multiple Labels</title>
      <link>https://egavves.github.io/publication/liao-2021-quasibinary/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/liao-2021-quasibinary/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rotation Equivariant Siamese Networks for Tracking</title>
      <link>https://egavves.github.io/publication/gupta-2021-rotation/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gupta-2021-rotation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Roto-translated Local Coordinate Frames For Interacting Dynamical Systems</title>
      <link>https://egavves.github.io/publication/kofinas-2021-roto/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/kofinas-2021-roto/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-selective context for interaction recognition</title>
      <link>https://egavves.github.io/publication/kilickaya-2021-self/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/kilickaya-2021-self/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse-Shot Learning for Extremely Many Localisations</title>
      <link>https://egavves.github.io/publication/panteli-2021-sparse/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/panteli-2021-sparse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spectral Smoothing Unveils Phase Transitions in Hierarchical Variational Autoencoders</title>
      <link>https://egavves.github.io/publication/pervez-2021-spectral/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/pervez-2021-spectral/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tackling occlusion in Siamese tracking with structured dropouts</title>
      <link>https://egavves.github.io/publication/gupta-2021-tackling/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gupta-2021-tackling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transformers for Ischemic Stroke Infarct Core Seentation from Spatio-temporal CT Perfusion Scans</title>
      <link>https://egavves.github.io/publication/de-2021-transformers/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/de-2021-transformers/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unsharp Mask Guided Filtering</title>
      <link>https://egavves.github.io/publication/shi-2021-unsharp/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/shi-2021-unsharp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Welcome to Wowchemy, the website builder for Hugo</title>
      <link>https://egavves.github.io/post/getting-started/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/post/getting-started/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;
















&lt;figure  id=&#34;figure-the-template-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/master/academic.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The template is mobile first with a responsive design to ensure that your site looks stunning on every device.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üëâ &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìö &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üí¨ &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üê¶ Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üí° &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;‚¨ÜÔ∏è &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Tutorial&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomplans&#34;&gt;&lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ‚ù§Ô∏è&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features ü¶Ñ‚ú®&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic CLI&lt;/a&gt;:&lt;/strong&gt; Automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatic triage of 12-lead ECGs using deep convolutional neural networks</title>
      <link>https://egavves.github.io/publication/vandeleur-2020-automatic/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/vandeleur-2020-automatic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Categorical normalizing flows via continuous transformations</title>
      <link>https://egavves.github.io/publication/lippe-2020-categorical/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/lippe-2020-categorical/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Low Bias Low Variance Gradient Estimates for Hierarchical Boolean Stochastic Networks</title>
      <link>https://egavves.github.io/publication/pervez-2020-low/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/pervez-2020-low/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pic: Permutation invariant convolution for recognizing long-range activities</title>
      <link>https://egavves.github.io/publication/hussein-2020-pic/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/hussein-2020-pic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pointmixup: Auentation for point clouds</title>
      <link>https://egavves.github.io/publication/chen-2020-pointmixup/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/chen-2020-pointmixup/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Siamese tracking of cell behaviour patterns</title>
      <link>https://egavves.github.io/publication/panteli-2020-siamese/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/panteli-2020-siamese/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Variance Reduction in Hierarchical Variational Autoencoders</title>
      <link>https://egavves.github.io/publication/pervez-2020-variance/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/pervez-2020-variance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3d neighborhood convolution: Learning depth-aware features for rgb-d and rgb semantic seentation</title>
      <link>https://egavves.github.io/publication/chen-20193-d/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/chen-20193-d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Combinatorial bayesian optimization using the graph cartesian product</title>
      <link>https://egavves.github.io/publication/oh-2019-combinatorial/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/oh-2019-combinatorial/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I bet you are wrong: Gambling adversarial networks for structured semantic seentation</title>
      <link>https://egavves.github.io/publication/samson-2019-bet/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/samson-2019-bet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Increasing Expressivity of a Hyperspherical VAE</title>
      <link>https://egavves.github.io/publication/davidson-2019-increasing/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/davidson-2019-increasing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Initialized Equilibrium Propagation for Backprop-Free Training</title>
      <link>https://egavves.github.io/publication/oconnor-2019-initialized/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/oconnor-2019-initialized/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SafeCritic: Collision-aware trajectory prediction</title>
      <link>https://egavves.github.io/publication/vanderheiden-2019-safecritic/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/vanderheiden-2019-safecritic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spherical regression: Learning viewpoints, surface normals and 3d rotations on n-spheres</title>
      <link>https://egavves.github.io/publication/liao-2019-spherical/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/liao-2019-spherical/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The seventh visual object tracking vot2019 challenge results</title>
      <link>https://egavves.github.io/publication/kristan-2019-seventh/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/kristan-2019-seventh/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Timeception for complex action recognition</title>
      <link>https://egavves.github.io/publication/hussein-2019-timeception/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/hussein-2019-timeception/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Training a spiking neural network with equilibrium propagation</title>
      <link>https://egavves.github.io/publication/oconnor-2019-training/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/oconnor-2019-training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Videograph: Recognizing minutes-long human activities in videos</title>
      <link>https://egavves.github.io/publication/hussein-2019-videograph/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/hussein-2019-videograph/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BOCK: Bayesian Optimization with Cylindrical Kernels</title>
      <link>https://egavves.github.io/publication/oh-2018-bock/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/oh-2018-bock/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic Adaptation on Non-Stationary Visual Domains</title>
      <link>https://egavves.github.io/publication/shkodrani-2018-dynamic/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/shkodrani-2018-dynamic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Improving word embedding compositionality using lexicographic definitions</title>
      <link>https://egavves.github.io/publication/scheepers-2018-improving/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/scheepers-2018-improving/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Long-term tracking in the wild: A benchmark</title>
      <link>https://egavves.github.io/publication/valmadre-2018-long/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/valmadre-2018-long/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Relaxed quantization for discretized neural networks</title>
      <link>https://egavves.github.io/publication/louizos-2018-relaxed/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/louizos-2018-relaxed/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Searching and Matching Texture-free 3D Shapes in Images</title>
      <link>https://egavves.github.io/publication/liao-2018-searching/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/liao-2018-searching/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The sixth visual object tracking vot2018 challenge results</title>
      <link>https://egavves.github.io/publication/kristan-2018-sixth/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/kristan-2018-sixth/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Training a network of spiking neurons with equilibrium propagation</title>
      <link>https://egavves.github.io/publication/oconnor-2018-training/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/oconnor-2018-training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Video Time: Properties, Encoders and Evaluation</title>
      <link>https://egavves.github.io/publication/ghodrati-2018-video/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/ghodrati-2018-video/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Videolstm convolves, attends and flows for action recognition</title>
      <link>https://egavves.github.io/publication/li-2018-videolstm/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/li-2018-videolstm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Action recognition with dynamic image networks</title>
      <link>https://egavves.github.io/publication/bilen-2017-action/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/bilen-2017-action/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reflectance and natural illumination from single-material specular objects using deep learning</title>
      <link>https://egavves.github.io/publication/georgoulis-2017-reflectance/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/georgoulis-2017-reflectance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-Supervised Video Representation Learning With Odd-One-Out Networks</title>
      <link>https://egavves.github.io/publication/fernando-2017-self/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/fernando-2017-self/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Temporally Efficient Deep Learning with Spikes</title>
      <link>https://egavves.github.io/publication/oconnor-2017-temporally/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/oconnor-2017-temporally/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tracking by natural language specification</title>
      <link>https://egavves.github.io/publication/li-2017-tracking/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/li-2017-tracking/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tracking for half an hour</title>
      <link>https://egavves.github.io/publication/tao-2017-tracking/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/tao-2017-tracking/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unified embedding and metric learning for zero-exemplar event detection</title>
      <link>https://egavves.github.io/publication/hussein-2017-unified/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/hussein-2017-unified/</guid>
      <description></description>
    </item>
    
    <item>
      <title>University of Amsterdam and Renmin University at TRECVID 2017: Searching Video, Detecting Events and Describing Video.</title>
      <link>https://egavves.github.io/publication/snoek-2017-university/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/snoek-2017-university/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automatic comment generation using a neural translation model</title>
      <link>https://egavves.github.io/publication/haije-2016-automatic/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/haije-2016-automatic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep reflectance maps</title>
      <link>https://egavves.github.io/publication/rematas-2016-deep/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/rematas-2016-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic image networks for action recognition</title>
      <link>https://egavves.github.io/publication/bilen-2016-dynamic/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/bilen-2016-dynamic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Online action detection</title>
      <link>https://egavves.github.io/publication/de-2016-online/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/de-2016-online/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rank pooling for action recognition</title>
      <link>https://egavves.github.io/publication/fernando-2016-rank/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/fernando-2016-rank/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Siamese instance search for tracking</title>
      <link>https://egavves.github.io/publication/tao-2016-siamese/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/tao-2016-siamese/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active transfer learning with zero-shot priors: Reusing past datasets for future tasks</title>
      <link>https://egavves.github.io/publication/gavves-2015-active/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gavves-2015-active/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Guiding the long-short term memory model for image caption generation</title>
      <link>https://egavves.github.io/publication/jia-2015-guiding/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/jia-2015-guiding/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning to rank based on subsequences</title>
      <link>https://egavves.github.io/publication/fernando-2015-learning/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/fernando-2015-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Local alignments for fine-grained categorization</title>
      <link>https://egavves.github.io/publication/gavves-2015-local/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gavves-2015-local/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling video evolution for action recognition</title>
      <link>https://egavves.github.io/publication/fernando-2015-modeling/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/fernando-2015-modeling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Attributes make sense on seented objects</title>
      <link>https://egavves.github.io/publication/li-2014-attributes/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/li-2014-attributes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Conceptlets: Selective semantics for classifying video events</title>
      <link>https://egavves.github.io/publication/mazloom-2014-conceptlets/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/mazloom-2014-conceptlets/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Costa: Co-occurrence statistics for zero-shot classification</title>
      <link>https://egavves.github.io/publication/mensink-2014-costa/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/mensink-2014-costa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Locality in generic instance search from one example</title>
      <link>https://egavves.github.io/publication/tao-2014-locality/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/tao-2014-locality/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://egavves.github.io/example-publication/example/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/example-publication/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Codemaps-seent, classify and search objects locally</title>
      <link>https://egavves.github.io/publication/li-2013-codemaps/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/li-2013-codemaps/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fine-grained categorization by alignments</title>
      <link>https://egavves.github.io/publication/gavves-2013-fine/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gavves-2013-fine/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Searching informative concept banks for video event detection</title>
      <link>https://egavves.github.io/publication/mazloom-2013-searching/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/mazloom-2013-searching/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Convex reduction of high-dimensional kernels for visual classification</title>
      <link>https://egavves.github.io/publication/gavves-2012-convex/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gavves-2012-convex/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visual synonyms for landmark image retrieval</title>
      <link>https://egavves.github.io/publication/gavves-2012-visual/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gavves-2012-visual/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Personalizing automated image annotation using cross-entropy</title>
      <link>https://egavves.github.io/publication/li-2011-personalizing/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/li-2011-personalizing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Landmark image retrieval using visual synonyms</title>
      <link>https://egavves.github.io/publication/gavves-2010-landmark/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/gavves-2010-landmark/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The MediaMill TRECVID 2010 semantic video search engine</title>
      <link>https://egavves.github.io/publication/snoek-2010-mediamill/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/publication/snoek-2010-mediamill/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://egavves.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://egavves.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
